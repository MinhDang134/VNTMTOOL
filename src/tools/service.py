# src/tools/service.py

import os
import uuid
from typing import List, Optional, Callable
from urllib.parse import urlparse, unquote

import httpx
from bs4 import BeautifulSoup
from datetime import datetime, timezone  # ÄÃ£ thÃªm timezone, bá» timedelta náº¿u khÃ´ng dÃ¹ng
import asyncio
from src.tools.models import Brand
from src.tools.config import settings  # Import Ä‘á»‘i tÆ°á»£ng settings Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o
from sqlmodel import Session, select
import logging
from src.tools.database import bulk_create, ensure_partition_exists
import random

from src.tools.state_manager import logging

# from src.tools.database import create_monthly_partitions # XÃ³a náº¿u khÃ´ng dÃ¹ng

# Sá»­ dá»¥ng trá»±c tiáº¿p thuá»™c tÃ­nh tá»« Ä‘á»‘i tÆ°á»£ng settings
LOCAL_MEDIA_BASE_URL = settings.LOCAL_MEDIA_BASE_URL
SOURCE_WEBSITE_DOMAIN = settings.SOURCE_WEBSITE_DOMAIN


class ScraperService:
    def __init__(self):
        self.proxy_index = 0
        self.request_count = 0
        self.last_request_time = datetime.now()
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Connection": "keep-alive",
            "DNT": "1",
            "Upgrade-Insecure-Requests": "1"
        }
        # ThÃªm láº¡i self.project_root_dir
        self.project_root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))

    def get_next_proxy(self) -> Optional[str]:
        if not settings.PROXY_IPS or not settings.PROXY_PORTS:
            logging.debug("proxy hoáº·c ip rá»—ng . cháº¡y khÃ´ng cÃ³ proxy.")
            return None

        # settings.PROXY_USERNAME vÃ  settings.PROXY_PASSWORD lÃ  Optional[str]
        # nÃªn chÃºng cÃ³ thá»ƒ lÃ  None.
        has_auth = settings.PROXY_USERNAME and settings.PROXY_PASSWORD

        if not (len(settings.PROXY_IPS) == len(settings.PROXY_PORTS)):
            logging.error("lá»—i config proxy: Ä‘á»™ dÃ i cá»§a proxy khÃ´ng trÃ¹ng .")
            return None

        proxy_ip = settings.PROXY_IPS[self.proxy_index]
        # PROXY_PORTS lÃ  List[int], khÃ´ng cáº§n chuyá»ƒn Ä‘á»•i ná»¯a
        proxy_port = settings.PROXY_PORTS[self.proxy_index]

        if has_auth:
            proxy_str = f"socks5://{settings.PROXY_USERNAME}:{settings.PROXY_PASSWORD}@{proxy_ip}:{proxy_port}"
        else:
            proxy_str = f"socks5://{proxy_ip}:{proxy_port}"

        self.proxy_index = (self.proxy_index + 1) % len(settings.PROXY_IPS)
        logging.debug(f"dÃ¹ng proxy sá»‘ : {proxy_ip}:{proxy_port}")
        return proxy_str

    async def download_image(self,image_url_original: str,base_save_path_on_disk: str = "media_root",image_subfolder: str = "brand_images"  ) -> str | None:
        if not image_url_original:
            logging.warning("download_image gá»i vá»›i má»™t link áº£nh gá»‘c .")
            return None

        # Táº¡o Ä‘Æ°á»ng dáº«n thÆ° má»¥c lÆ°u áº£nh trÃªn Ä‘Ä©a
        # self.project_root_dir lÃ  thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n cá»§a báº¡n
        full_save_folder_on_disk = os.path.join(self.project_root_dir, base_save_path_on_disk, image_subfolder)
        try:
            os.makedirs(full_save_folder_on_disk, exist_ok=True)
        except OSError as e:
            logging.error(f"khÃ´ng thá»ƒ táº¡o má»™t direction  {full_save_folder_on_disk}: {e}")
            return None

        try:
            # Thay tháº¿ settings.SSL_VERIFY_DOWNLOAD vÃ  settings.DOWNLOAD_TIMEOUT báº±ng giÃ¡ trá»‹ thá»±c táº¿ hoáº·c config cá»§a báº¡n
            ssl_verify = getattr(settings, 'SSL_VERIFY_DOWNLOAD', True)
            download_timeout = getattr(settings, 'DOWNLOAD_TIMEOUT', 30.0)

            async with httpx.AsyncClient(verify=ssl_verify, timeout=download_timeout,
                                         follow_redirects=True) as client:
                logging.info(f"Äang cá»‘ gáº¯ng táº£i xuá»‘ng hÃ¬nh áº£nh tá»«: {image_url_original}")
                img_response = await client.get(image_url_original, headers=self.headers)
                img_response.raise_for_status()  # NÃ©m lá»—i náº¿u status code lÃ  4xx hoáº·c 5xx

                # --- Báº¯t Ä‘áº§u logic xÃ¡c Ä‘á»‹nh pháº§n má»Ÿ rá»™ng file ---
                parsed_url = urlparse(image_url_original)
                path_component = unquote(parsed_url.path)
                original_filename_from_url = os.path.basename(path_component)
                _, ext_from_url = os.path.splitext(original_filename_from_url)

                logging.debug(f"DEBUG download_image: URL gá»‘c: '{image_url_original}'")
                logging.debug(f"DEBUG download_image: TÃªn tá»‡p gá»‘c tá»« URL: '{original_filename_from_url}'")
                logging.debug(f"DEBUG download_image: Má»Ÿ rá»™ng tá»« URL: '{ext_from_url}' (repr: {repr(ext_from_url)})")

                content_type = img_response.headers.get("content-type", "").lower()
                logging.debug(f"DEBUG download_image: TiÃªu Ä‘á» Content-Type: '{content_type}'")

                # Æ¯u tiÃªn xÃ¡c Ä‘á»‹nh pháº§n má»Ÿ rá»™ng tá»« Content-Type
                determined_ext = None
                if "image/jpeg" in content_type or "image/jpg" in content_type:
                    determined_ext = ".jpg"
                elif "image/png" in content_type:
                    determined_ext = ".png"
                elif "image/gif" in content_type:
                    determined_ext = ".gif"
                elif "image/webp" in content_type:
                    determined_ext = ".webp"
                elif "image/svg+xml" in content_type:
                    determined_ext = ".svg"

                if not determined_ext:
                    logging.warning(
                        f"KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh pháº§n má»Ÿ rá»™ng chuáº©n tá»« Content-Type '{content_type}' for {image_url_original}. "
                        f"Pháº§n má»Ÿ rá»™ng gá»‘c tá»« URL lÃ  '{ext_from_url}'. Máº·c Ä‘á»‹nh lÃ  .jpg nhÆ° má»™t giáº£i phÃ¡p dá»± phÃ²ng.")
                    determined_ext = ".jpg"  # Máº·c Ä‘á»‹nh an toÃ n lÃ  .jpg náº¿u khÃ´ng rÃµ

                logging.debug(
                    f"DEBUG download_image: Pháº§n má»Ÿ rá»™ng cuá»‘i cÃ¹ng Ä‘Æ°á»£c chá»n: '{determined_ext}' (repr: {repr(determined_ext)})")
                # --- Káº¿t thÃºc logic xÃ¡c Ä‘á»‹nh pháº§n má»Ÿ rá»™ng file ---

                unique_filename_base = str(uuid.uuid4())
                # Sá»­ dá»¥ng determined_ext Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a
                unique_filename = f"{unique_filename_base}{determined_ext}"
                logging.debug(
                    f"DEBUG download_image: TÃªn tá»‡p duy nháº¥t Ä‘Æ°á»£c táº¡o:'{unique_filename}' (repr: {repr(unique_filename)})")

                save_path_on_disk = os.path.join(full_save_folder_on_disk, unique_filename)

                with open(save_path_on_disk, "wb") as f:
                    f.write(img_response.content)

                # ÄÆ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i nÃ y sáº½ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ táº¡o URL truy cáº­p áº£nh
                relative_url_path = os.path.join(image_subfolder, unique_filename).replace("\\", "/")
                logging.info(
                    f"HÃ¬nh áº£nh Ä‘Ã£ Ä‘Æ°á»£c táº£i xuá»‘ng thÃ nh cÃ´ng: {save_path_on_disk}. Pháº§n URL tÆ°Æ¡ng Ä‘á»‘i: {relative_url_path}")
                logging.debug(
                    f"DEBUG download_image: ÄÆ°á»ng dáº«n URL tÆ°Æ¡ng Ä‘á»‘i cáº§n tráº£ vá»: '{relative_url_path}' (repr: {repr(relative_url_path)})")
                return relative_url_path

        except httpx.HTTPStatusError as e_http:
            error_text = e_http.response.text if hasattr(e_http, 'response') and e_http.response and hasattr(
                e_http.response, 'text') else str(e_http)
            status_code_text = e_http.response.status_code if hasattr(e_http, 'response') and hasattr(e_http.response,
                                                                                                      'status_code') else 'N/A'
            logging.error(f"HTTP lá»—i  {status_code_text} táº£i image {image_url_original}: {error_text}")
            return None
        except httpx.RequestError as e_req:
            logging.error(f"YÃªu cáº§u táº£i xuá»‘ng hÃ¬nh áº£nh lá»—i {image_url_original}: {str(e_req)}")
            return None
        except Exception as e:
            logging.error(f"Lá»—i chung khi táº£i hÃ¬nh áº£nh {image_url_original}: {str(e)}", exc_info=True)
            return None

    async def make_request(self, url: str, max_retries: Optional[int] = None) -> Optional[httpx.Response]:
        effective_max_retries = max_retries if max_retries is not None else settings.MAX_REQUEST_RETRIES

        current_proxy = self.get_next_proxy()
        proxies_config = {"http://": current_proxy, "https://": current_proxy} if current_proxy else None

        for attempt in range(effective_max_retries):
            try:
                min_delay_req = settings.MIN_REQUEST_DELAY
                max_delay_req = settings.MAX_REQUEST_DELAY
                if attempt > 0:
                    await asyncio.sleep(random.uniform(min_delay_req, max_delay_req))

                async with httpx.AsyncClient(
                        timeout=httpx.Timeout(settings.REQUEST_TIMEOUT),
                        verify=settings.SSL_VERIFY_REQUEST,
                        follow_redirects=True,
                        proxies=proxies_config
                ) as client:
                    logging.debug(
                        f"Making request to {url} (Attempt {attempt + 1}/{effective_max_retries}) with proxy {current_proxy or 'None'}")
                    response = await client.get(url, headers=self.headers)
                    response.raise_for_status()
                    return response

            except httpx.HTTPStatusError as e_http:
                error_text = e_http.response.text if hasattr(e_http, 'response') and e_http.response and hasattr(
                    e_http.response, 'text') else str(e_http)
                status_code = e_http.response.status_code if hasattr(e_http, 'response') and hasattr(e_http.response,
                                                                                                     'status_code') else None
                logging.warning(
                    f"Lá»—i tráº¡ng thÃ¡i HTTP (Cá»‘ gáº¯ng {attempt + 1}/{effective_max_retries}) for {url}: {status_code or 'N/A'} - {error_text}")

                if status_code in [403, 401, 429]:
                    logging.error(
                        f"Lá»—i HTTP nghiÃªm trá»ng {status_code} for {url}. Thay Ä‘á»•i proxy vÃ  thá»­ láº¡i náº¿u cÃ³ thá»ƒ.")
                    current_proxy = self.get_next_proxy()
                    proxies_config = {"http://": current_proxy, "https://": current_proxy} if current_proxy else None
                    if attempt == effective_max_retries - 1: return None
                    await asyncio.sleep(random.uniform(5, 10))
                    continue
                if attempt == effective_max_retries - 1: return None
                await asyncio.sleep(random.uniform(2, 5))

            except httpx.RequestError as e_req:
                logging.warning(
                    f"YÃªu cáº§u Lá»—i (Cá»‘ gáº¯ng {attempt + 1}/{effective_max_retries}) for {url}: {str(e_req)}")
                current_proxy = self.get_next_proxy()
                proxies_config = {"http://": current_proxy, "https://": current_proxy} if current_proxy else None
                if attempt == effective_max_retries - 1: return None
                await asyncio.sleep(random.uniform(3, 7))

            except Exception as e_generic:
                logging.error(
                    f"Lá»—i chung (Cá»‘ gáº¯ng {attempt + 1}/{effective_max_retries}) for {url}: {str(e_generic)}",
                    exc_info=True)
                if attempt == effective_max_retries - 1: return None
                await asyncio.sleep(random.uniform(2, 5))
        logging.error(f"All {effective_max_retries} thá»­ láº¡i khÃ´ng thÃ nh cÃ´ng cho URL: {url}")
        return None

    async def scrape_by_date_range(self,start_date: datetime,end_date: datetime,session: Session,initial_start_page: int,state_save_callback: Callable[[int], None]) -> List[Brand]:  # Tráº£ vá» danh sÃ¡ch cÃ¡c Brand Ä‘Ã£ xá»­ lÃ½ trong láº§n cháº¡y nÃ y

        current_page = initial_start_page
        brands_collected_in_this_run: List[Brand] = []
        stop_scraping_due_to_duplicate_policy = False

        # CÃ¡c biáº¿n cáº¥u hÃ¬nh tá»« settings (giá»¯ nguyÃªn)
        request_limit_per_interval = settings.REQUEST_LIMIT_PER_INTERVAL
        request_interval_seconds = settings.REQUEST_INTERVAL_SECONDS
        min_request_delay = settings.MIN_REQUEST_DELAY
        max_request_delay = settings.MAX_REQUEST_DELAY


        while True:  # Láº·p qua cÃ¡c trang
            # Xá»­ lÃ½ giá»›i háº¡n request (giá»¯ nguyÃªn)
            if self.request_count >= request_limit_per_interval:
                time_diff = datetime.now() - self.last_request_time
                if time_diff.total_seconds() < request_interval_seconds:
                    sleep_duration = request_interval_seconds - time_diff.total_seconds()
                    logging.info(f"Äáº¡t giá»›i háº¡n request. Nghá»‰ {sleep_duration:.2f} giÃ¢y.")
                    await asyncio.sleep(sleep_duration)
                self.request_count = 0
                self.last_request_time = datetime.now()

            start_str = start_date.strftime("%d.%m.%Y")
            end_str = end_date.strftime("%d.%m.%Y")
            url = f"https://vietnamtrademark.net/search?fd={start_str}%20-%20{end_str}&p={current_page}"

            logging.info(f"Äang cÃ o trang: {current_page} cho URL: {url}")
            response = await self.make_request(url)
            self.request_count += 1

            if not response:
                logging.error(
                    f"KhÃ´ng nháº­n Ä‘Æ°á»£c pháº£n há»“i cho trang {current_page} (URL: {url}). Dá»«ng xá»­ lÃ½ khoáº£ng ngÃ y nÃ y.")
                break

            try:
                soup = BeautifulSoup(response.text, 'html.parser')
            except Exception as e_soup:
                logging.error(f"Lá»—i khi parse HTML cho trang {current_page}: {e_soup}", exc_info=True)
                break

            rows = soup.select("table.table tbody tr")
            if not rows:
                logging.info(
                    f"KhÃ´ng tÃ¬m tháº¥y hÃ ng (dá»¯ liá»‡u) nÃ o trÃªn trang {current_page}. Káº¿t thÃºc cho khoáº£ng ngÃ y nÃ y.")
                break

            brands_extracted_from_this_page: List[Brand] = []  # LÆ°u cÃ¡c brand tá»« trang hiá»‡n táº¡i
            page_had_new_valid_data = False  # Cá» Ä‘á»ƒ biáº¿t trang nÃ y cÃ³ dá»¯ liá»‡u má»›i há»£p lá»‡ khÃ´ng

            for row_idx, row in enumerate(rows):
                try:
                    # --- Báº®T Äáº¦U LOGIC TRÃCH XUáº¤T Dá»® LIá»†U Tá»ª 1 HÃ€NG (ROW) ---
                    # (ÄÃ¢y lÃ  pháº§n code gá»‘c cá»§a báº¡n, tÃ´i chá»‰ tÃ³m táº¯t láº¡i cÃ¡c trÆ°á»ng cáº§n láº¥y)
                    date_text_tag = row.select_one("td:nth-child(7)")
                    if not date_text_tag or not date_text_tag.text.strip():
                        logging.warning(f"HÃ ng {row_idx + 1} trang {current_page}: Thiáº¿u ngÃ y ná»™p Ä‘Æ¡n. Bá» qua hÃ ng.")
                        continue
                    try:
                        parsed_application_date = datetime.strptime(date_text_tag.text.strip(), "%d.%m.%Y").date()
                    except ValueError as ve:
                        logging.warning(
                            f"HÃ ng {row_idx + 1} trang {current_page}: Lá»—i parse ngÃ y '{date_text_tag.text.strip()}': {ve}. Bá» qua hÃ ng.")
                        continue

                    # Äáº£m báº£o partition tá»“n táº¡i cho ngÃ y nÃ y (quan trá»ng cho insert sau nÃ y)
                    ensure_partition_exists(parsed_application_date)  # Gá»i hÃ m nÃ y tá»« database.py

                    brand_name_tag = row.select_one("td:nth-child(4) label")
                    brand_name = brand_name_tag.text.strip() if brand_name_tag else ""

                    image_tag = row.select_one("td.mau-nhan img")
                    image_url_original_src = image_tag["src"] if image_tag and image_tag.has_attr("src") else None
                    final_image_url_for_db = None
                    if image_url_original_src:
                        current_image_url_to_download = image_url_original_src
                        if current_image_url_to_download.startswith("/"):
                            current_image_url_to_download = f"{SOURCE_WEBSITE_DOMAIN.rstrip('/')}{current_image_url_to_download}"
                        # ... (cÃ¡c xá»­ lÃ½ URL khÃ¡c náº¿u cáº§n) ...
                        saved_relative_image_path = await self.download_image(current_image_url_to_download)
                        if saved_relative_image_path:
                            final_image_url_for_db = f"{LOCAL_MEDIA_BASE_URL.rstrip('/')}/{saved_relative_image_path.lstrip('/')}"

                    product_group_tag = row.select_one("td:nth-child(5) span")
                    product_group = product_group_tag.text.strip() if product_group_tag else ""

                    status_tag = row.select_one("td.trang-thai span.badge")
                    status = status_tag.text.strip() if status_tag else ""

                    application_number_tag = row.select_one("td:nth-child(8) a")
                    application_number = application_number_tag.text.strip() if application_number_tag else ""
                    if not application_number:
                        logging.warning(f"HÃ ng {row_idx + 1} trang {current_page}: Thiáº¿u sá»‘ Ä‘Æ¡n. Bá» qua hÃ ng.")
                        continue

                    applicant_tag = row.select_one("td:nth-child(9)")
                    applicant = applicant_tag.text.strip() if applicant_tag else ""

                    representative_tag = row.select_one("td:nth-child(10)")
                    representative = representative_tag.text.strip() if representative_tag else ""

                    stmt = select(Brand).where(Brand.application_number == application_number)
                    existing_brand = session.exec(stmt).first()

                    if existing_brand:
                        logging.info(
                            f"Brand vá»›i sá»‘ Ä‘Æ¡n {application_number} (trang {current_page}) Ä‘Ã£ tá»“n táº¡i trong DB. Bá» qua.")
                        continue

                    # Táº¡o Ä‘á»‘i tÆ°á»£ng Brand
                    brand_obj = Brand(
                        brand_name=brand_name,
                        image_url=final_image_url_for_db,
                        product_group=product_group,
                        status=status,
                        application_date=parsed_application_date,
                        application_number=application_number,
                        applicant=applicant,
                        representative=representative
                        # updated_at sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng bá»Ÿi DB hoáº·c SQLModel default
                    )
                    brands_extracted_from_this_page.append(brand_obj)
                    page_had_new_valid_data = True  # ÄÃ¡nh dáº¥u trang nÃ y cÃ³ dá»¯ liá»‡u má»›i cáº§n lÆ°u

                except Exception as e_row_processing:
                    row_html_snippet = str(row)[:250]  # Láº¥y má»™t Ä‘oáº¡n HTML Ä‘á»ƒ debug
                    logging.error(f"Lá»—i xá»­ lÃ½ hÃ ng {row_idx + 1} trÃªn trang {current_page}: {e_row_processing}\n"
                                 f"HTML Snippet: {row_html_snippet}", exc_info=True)
                    continue


            # Xá»­ lÃ½ lÆ°u dá»¯ liá»‡u cá»§a trang hiá»‡n táº¡i vÃ o DB
            if brands_extracted_from_this_page:
                logging.info(
                    f"Trang {current_page}: TrÃ­ch xuáº¥t Ä‘Æ°á»£c {len(brands_extracted_from_this_page)} nhÃ£n hiá»‡u má»›i.")
                try:
                    bulk_create(session, brands_extracted_from_this_page)
                    session.commit()
                    logging.info(
                        f"ÄÃƒ COMMIT THÃ€NH CÃ”NG {len(brands_extracted_from_this_page)} nhÃ£n hiá»‡u tá»« trang {current_page} vÃ o DB.")
                    brands_collected_in_this_run.extend(brands_extracted_from_this_page)
                    state_save_callback(current_page)

                except Exception as e_db_commit:
                    logging.error(f"Lá»—i khi thÃªm hoáº·c commit dá»¯ liá»‡u cho trang {current_page} vÃ o DB: {e_db_commit}",
                                 exc_info=True)
                    try:
                        session.rollback()  # Quan trá»ng: Rollback láº¡i náº¿u commit lá»—i
                        logging.info(f"ÄÃ£ rollback transaction cho trang {current_page} do lá»—i.")
                    except Exception as e_rollback:
                        logging.error(
                            f"Lá»—i nghiÃªm trá»ng khi rollback transaction cho trang {current_page}: {e_rollback}",
                            exc_info=True)
                    break

            elif page_had_new_valid_data is False and rows:  # Trang cÃ³ rows nhÆ°ng khÃ´ng cÃ³ data má»›i (vÃ­ dá»¥: toÃ n bá»™ Ä‘Ã£ tá»“n táº¡i hoáº·c bá»‹ skip)
                logging.info(f"Trang {current_page} Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ nhÆ°ng khÃ´ng cÃ³ dá»¯ liá»‡u má»›i nÃ o Ä‘Æ°á»£c thÃªm vÃ o DB.")
                state_save_callback(current_page)
            current_page += 1
            await asyncio.sleep(random.uniform(min_request_delay, max_request_delay))


        logging.info(
            f"Káº¿t thÃºc scrape_by_date_range. Tá»•ng sá»‘ nhÃ£n hiá»‡u Ä‘Æ°á»£c thu tháº­p vÃ  LÆ¯U THÃ€NH CÃ”NG trong láº§n cháº¡y nÃ y: {len(brands_collected_in_this_run)}.")
        return brands_collected_in_this_run

    async def check_pending_brands(self, session: Session):
        # Sá»­ dá»¥ng logger cá»§a class hoáº·c module, hoáº·c láº¥y logger má»›i
        logger = logging.getLogger(f"{self.__class__.__name__}.check_pending_brands")

        logger.info("Báº¯t Ä‘áº§u kiá»ƒm tra cÃ¡c Ä‘Æ¡n cÃ³ tráº¡ng thÃ¡i 'Ä‘ang giáº£i quyáº¿t'...")

        # 1. Truy váº¥n dá»¯ liá»‡u tá»« há»‡ thá»‘ng ná»™i bá»™
        # Äiá»u kiá»‡n: status == "Ä‘ang giáº£i quyáº¿t" (theo yÃªu cáº§u)
        statement = select(Brand).where(Brand.status == "Ä‘ang giáº£i quyáº¿t")
        pending_brands: List[Brand] = session.exec(statement).all()

        if not pending_brands:
            logger.info("âœ… KhÃ´ng tÃ¬m tháº¥y Ä‘Æ¡n nÃ o cÃ³ tráº¡ng thÃ¡i 'Ä‘ang giáº£i quyáº¿t' Ä‘á»ƒ kiá»ƒm tra.")
            return

        logger.info(f"ğŸ” TÃ¬m tháº¥y {len(pending_brands)} Ä‘Æ¡n cÃ³ tráº¡ng thÃ¡i 'Ä‘ang giáº£i quyáº¿t' Ä‘á»ƒ kiá»ƒm tra.")
        updated_count = 0
        processed_count = 0

        # CÃ¢n nháº¯c thÃªm delay giá»¯a cÃ¡c request Ä‘á»ƒ trÃ¡nh lÃ m quÃ¡ táº£i server VietnamTrademark
        # min_delay_check = getattr(settings, 'MIN_DELAY_CHECK_PENDING', 1.0) # Láº¥y tá»« config hoáº·c máº·c Ä‘á»‹nh
        # max_delay_check = getattr(settings, 'MAX_DELAY_CHECK_PENDING', 3.0)

        for brand_idx, brand in enumerate(pending_brands):
            processed_count += 1
            logger.info(
                f"Äang xá»­ lÃ½ Ä‘Æ¡n {brand_idx + 1}/{len(pending_brands)}: ID {brand.id}, Sá»‘ Ä‘Æ¡n {brand.application_number}")

            # if brand_idx > 0: # ThÃªm delay náº¿u muá»‘n
            #     await asyncio.sleep(random.uniform(min_delay_check, max_delay_check))

            if not brand.application_number:
                logger.warning(f"âš ï¸ ÄÆ¡n cÃ³ ID {brand.id} khÃ´ng cÃ³ sá»‘ Ä‘Æ¡n (application_number). Bá» qua.")
                continue

            # 2. Gá»i API hoáº·c gá»­i HTTP request Ä‘áº¿n VietnamTrademark
            url = f"https://vietnamtrademark.net/search?q={brand.application_number.strip()}"
            logger.info(f"ğŸŒ Gá»i Ä‘áº¿n VietnamTrademark: {url}")

            response = await self.make_request(url)  # Sá»­ dá»¥ng láº¡i hÃ m make_request Ä‘Ã£ cÃ³

            if not response:
                logger.warning(
                    f"âŒ KhÃ´ng nháº­n Ä‘Æ°á»£c pháº£n há»“i tá»« VietnamTrademark cho sá»‘ Ä‘Æ¡n {brand.application_number} (ID: {brand.id}). Bá» qua Ä‘Æ¡n nÃ y.")
                continue

            try:
                # 3. PhÃ¢n tÃ­ch káº¿t quáº£ HTML
                soup = BeautifulSoup(response.text, 'html.parser')
                target_row = None
                # Selector cho báº£ng vÃ  cÃ¡c hÃ ng, dá»±a trÃªn cáº¥u trÃºc HTML cá»§a trang káº¿t quáº£
                rows_on_page = soup.select("table.table tbody tr")
                if not rows_on_page:
                    logger.warning(
                        f"ğŸ“„ KhÃ´ng tÃ¬m tháº¥y báº£ng/hÃ ng dá»¯ liá»‡u nÃ o trÃªn trang káº¿t quáº£ cho sá»‘ Ä‘Æ¡n {brand.application_number}.")
                    continue

                for r_check in rows_on_page:
                    # Selector cho cá»™t chá»©a sá»‘ Ä‘Æ¡n (vÃ­ dá»¥: cá»™t thá»© 8, tháº» a)
                    app_num_tag_check = r_check.select_one("td:nth-child(8) a")
                    if app_num_tag_check and app_num_tag_check.text.strip() == brand.application_number:
                        target_row = r_check
                        break

                if not target_row:
                    logger.warning(
                        f"ğŸ“„ KhÃ´ng tÃ¬m tháº¥y hÃ ng khá»›p vá»›i sá»‘ Ä‘Æ¡n {brand.application_number} trÃªn trang káº¿t quáº£ tÃ¬m kiáº¿m.")
                    continue

                # TrÃ­ch xuáº¥t status má»›i tá»« HTML (vÃ­ dá»¥: td class 'trang-thai', span class 'badge')
                status_tag = target_row.select_one("td.trang-thai span.badge")
                if status_tag:
                    new_status = status_tag.text.strip()
                    logger.info(
                        f"ğŸ“Š Tráº¡ng thÃ¡i má»›i tá»« web cho {brand.application_number}: '{new_status}' (Tráº¡ng thÃ¡i hiá»‡n táº¡i trong DB: '{brand.status}')")

                    # 4. So sÃ¡nh vÃ  xÃ¡c Ä‘á»‹nh Ä‘Æ¡n cáº§n cáº­p nháº­t
                    if new_status != brand.status:
                        old_status = brand.status
                        brand.status = new_status
                        brand.updated_at = datetime.now(timezone.utc)  # Cáº­p nháº­t thá»i gian
                        session.add(brand)  # ÄÆ°a vÃ o session Ä‘á»ƒ chuáº©n bá»‹ commit
                        updated_count += 1
                        logger.info(
                            f"ğŸ”„ Cáº¬P NHáº¬T: ÄÆ¡n {brand.application_number} (ID: {brand.id}) thay Ä‘á»•i tráº¡ng thÃ¡i tá»« '{old_status}' -> '{new_status}'")
                    else:
                        logger.info(
                            f"âœ… Tráº¡ng thÃ¡i cho Ä‘Æ¡n {brand.application_number} (ID: {brand.id}) khÃ´ng thay Ä‘á»•i ('{brand.status}').")
                else:
                    logger.warning(
                        f"ğŸ“„ KhÃ´ng tÃ¬m tháº¥y tháº» tráº¡ng thÃ¡i (status_tag) cho sá»‘ Ä‘Æ¡n {brand.application_number} (ID: {brand.id}) trong hÃ ng tÆ°Æ¡ng á»©ng.")

            except Exception as e_check:
                logger.error(
                    f"âŒ Lá»—i khi xá»­ lÃ½/bÃ³c tÃ¡ch tráº¡ng thÃ¡i cho Ä‘Æ¡n {brand.application_number} (ID: {brand.id}): {str(e_check)}",
                    exc_info=True)
                continue  # Bá» qua Ä‘Æ¡n nÃ y vÃ  tiáº¿p tá»¥c vá»›i Ä‘Æ¡n khÃ¡c

        # 5. Cáº­p nháº­t vÃ o database (sau khi Ä‘Ã£ duyá»‡t qua táº¥t cáº£ cÃ¡c Ä‘Æ¡n)
        if updated_count > 0:
            try:
                session.commit()
                logger.info(f"ğŸ’¾ ÄÃƒ COMMIT THÃ€NH CÃ”NG: Cáº­p nháº­t tráº¡ng thÃ¡i cho {updated_count} Ä‘Æ¡n vÃ o database.")
            except Exception as e_commit:
                logger.error(f"âŒ Lá»—i khi commit cÃ¡c thay Ä‘á»•i tráº¡ng thÃ¡i vÃ o database: {e_commit}", exc_info=True)
                session.rollback()  # Quan trá»ng: Rollback náº¿u cÃ³ lá»—i khi commit
                logger.info("ÄÃ£ rollback transaction do lá»—i commit.")
        elif processed_count > 0:  # ÄÃ£ xá»­ lÃ½ má»™t sá»‘ Ä‘Æ¡n nhÆ°ng khÃ´ng cÃ³ Ä‘Æ¡n nÃ o thay Ä‘á»•i tráº¡ng thÃ¡i
            logger.info("âœ… KhÃ´ng cÃ³ tráº¡ng thÃ¡i Ä‘Æ¡n nÃ o cáº§n cáº­p nháº­t sau khi kiá»ƒm tra toÃ n bá»™ danh sÃ¡ch.")

        logger.info(f"HoÃ n táº¥t kiá»ƒm tra. ÄÃ£ xá»­ lÃ½ {processed_count} Ä‘Æ¡n, cáº­p nháº­t {updated_count} Ä‘Æ¡n.")