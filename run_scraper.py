import asyncio
from datetime import datetime, timedelta, date as date_type
import os
from concurrent.futures import ProcessPoolExecutor, as_completed   
from multiprocessing import Manager   
from functools import partial   
from sqlmodel import create_engine   
from src.tools.config import settings
from src.tools.service import ScraperService
from src.tools.database import get_session, ensure_partition_exists, setup_database_schema
from src.tools.state_manager import (
    load_scrape_state, save_scrape_state,
    load_control_state, save_control_state, get_control_state_path,
    clear_page_state_for_day
)
import logging   
# ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi b·∫°n mu·ªën l∆∞u log
LOG_OUTPUT_DIR_PATH = "/home/minhdangpy134/Logvntmtool"


try:
    os.makedirs(LOG_OUTPUT_DIR_PATH, exist_ok=True)
except OSError as e:
    print(f"L·ªói khi t·∫°o th∆∞ m·ª•c log {LOG_OUTPUT_DIR_PATH}: {e}")



LOG_FILE_NAME = "scraper_activity2.txt"
LOG_FILE_PATH = os.path.join(LOG_OUTPUT_DIR_PATH, LOG_FILE_NAME)


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(processName)s (%(process)d) - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE_PATH, mode='a', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
MEDIA_PHYSICAL_DIR = os.path.join(PROJECT_ROOT, "media_root", "brand_images")
os.makedirs(MEDIA_PHYSICAL_DIR, exist_ok=True)
logging.info(f"Th∆∞ m·ª•c l∆∞u tr·ªØ media: {MEDIA_PHYSICAL_DIR}")

PAGE_STATE_FILE_PATH = os.path.join(PROJECT_ROOT, "scraper_page_state.json")
CONTROL_STATE_FILE_PATH = get_control_state_path(PROJECT_ROOT)

RUN_DURATION_SECONDS = settings.RUN_DURATION_MINUTES * 60
PAUSE_DURATION_SECONDS = settings.PAUSE_DURATION_MINUTES * 60
NUM_PROCESSES = settings.CONCURRENT_SCRAPING_TASKS


def scrape_day_worker(current_day_to_process: date_type,db_url: str,page_state_file_path: str,page_state_lock,media_physical_dir_worker: str):
    log = logging.getLogger(f"Worker-{current_day_to_process.strftime('%Y-%m-%d')}")   
    log.info(f"Worker b·∫Øt ƒë·∫ßu x·ª≠ l√Ω ng√†y: {current_day_to_process}")   

    try:   
        worker_engine = create_engine(db_url, pool_pre_ping=True)   

        scraper = ScraperService(   
            media_dir=media_physical_dir_worker,   
        )   

        day_key = f"brands_{current_day_to_process.strftime('%Y-%m-%d')}_{current_day_to_process.strftime('%Y-%m-%d')}"   

        def state_saver_callback_for_day_in_worker(page_just_completed: int):   
            save_scrape_state(page_state_file_path, day_key, page_just_completed, page_state_lock)   

        initial_page_for_this_day = load_scrape_state(page_state_file_path, day_key)   

        scrape_result = {}   
        loop = asyncio.new_event_loop()   
        asyncio.set_event_loop(loop)   
        try:   
            dt_for_partition = datetime.combine(current_day_to_process, datetime.min.time())
            ensure_partition_exists(dt_for_partition, worker_engine)

            with get_session(worker_engine) as session:   
                scrape_result = loop.run_until_complete(scraper.scrape_by_date_range(   
                    start_date=current_day_to_process,   
                    end_date=current_day_to_process,   
                    session=session,   
                    initial_start_page=initial_page_for_this_day,   
                    state_save_callback=state_saver_callback_for_day_in_worker   
                ))   
        finally:   
            loop.close()   

        log.info(
            f"Worker ho√†n th√†nh x·ª≠ l√Ω ng√†y {current_day_to_process} v·ªõi k·∫øt qu·∫£: {scrape_result.get('status')}")   
        return {"date": current_day_to_process, "result": scrape_result}   

    except Exception as e:   
        log.error(f"L·ªói nghi√™m tr·ªçng trong worker cho ng√†y {current_day_to_process}: {e}", exc_info=True)   
        return {"date": current_day_to_process,
                "result": {"status": "worker_crash", "message": str(e), "brands_processed_count": 0}}   
    finally:   
        if 'worker_engine' in locals() and worker_engine:   
            worker_engine.dispose()   


def get_next_day_to_process() -> date_type:
    control_state = load_control_state(CONTROL_STATE_FILE_PATH)
    last_completed_str = control_state.get("last_fully_completed_day")

    if last_completed_str:
        try:
            last_completed_date = datetime.strptime(last_completed_str, "%Y-%m-%d").date()
            next_day = last_completed_date + timedelta(days=1)
            logging.info(
                f"Ng√†y cu·ªëi c√πng ho√†n th√†nh ƒë∆∞·ª£c ghi nh·∫≠n: {last_completed_str}. Ng√†y ti·∫øp theo ƒë·ªÉ x·ª≠ l√Ω: {next_day.strftime('%Y-%m-%d')}")
            return next_day
        except ValueError:
            logging.warning(
                f"ƒê·ªãnh d·∫°ng ng√†y trong file tr·∫°ng th√°i ƒëi·ªÅu khi·ªÉn kh√¥ng h·ª£p l·ªá: '{last_completed_str}'. S·∫Ω b·∫Øt ƒë·∫ßu t·ª´ ng√†y c·∫•u h√¨nh.")

    start_date = date_type(
        settings.INITIAL_SCRAPE_START_YEAR,
        settings.INITIAL_SCRAPE_START_MOTH,
        settings.INITIAL_SCRAPE_START_DAY
    )
    logging.info(
        f"Kh√¥ng c√≥ tr·∫°ng th√°i ng√†y ho√†n th√†nh h·ª£p l·ªá. B·∫Øt ƒë·∫ßu t·ª´ ng√†y c·∫•u h√¨nh m·∫∑c ƒë·ªãnh: {start_date.strftime('%Y-%m-%d')}")
    return start_date


def get_overall_end_date() -> date_type:
    if settings.OVERALL_SCRAPE_END_YEAR and settings.OVERALL_SCRAPE_END_MOTH and settings.OVERALL_SCRAPE_END_DAY:
        try:
            return date_type(
                settings.OVERALL_SCRAPE_END_YEAR,
                settings.OVERALL_SCRAPE_END_MOTH,
                settings.OVERALL_SCRAPE_END_DAY
            )
        except (TypeError, ValueError) as e:
            logging.warning(
                f"M·ªôt s·ªë gi√° tr·ªã OVERALL_SCRAPE_END trong settings kh√¥ng h·ª£p l·ªá ho·∫∑c thi·∫øu: {e}. S·∫Ω m·∫∑c ƒë·ªãnh l√† ng√†y h√¥m qua.")
    return datetime.now().date() - timedelta(days=1)


async def daily_scraping_manager():   
    logging.info("Kh·ªüi t·∫°o Scraping Manager v·ªõi Multiprocessing.")   

    with Manager() as manager:   
        page_state_lock = manager.Lock()   

        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:   
            current_day_to_process = get_next_day_to_process()   
            overall_end_date = get_overall_end_date()   

            active_futures = {}   

            while True:   
                logging.info(   
                    f"====== B·∫ÆT ƒê·∫¶U PHI√äN L√ÄM VI·ªÜC M·ªöI ({settings.RUN_DURATION_MINUTES} PH√öT) l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")   
                session_start_time_ns = asyncio.get_event_loop().time()   

                days_processed_in_this_session = 0   

                while (asyncio.get_event_loop().time() - session_start_time_ns) < RUN_DURATION_SECONDS:   
                    # completed_futures_dates = []   (D√≤ng n√†y kh√¥ng c·∫ßn thi·∫øt)
                    for future in as_completed(
                            list(active_futures.keys())):
                        processed_date = active_futures.pop(future)   
                        try:   
                            worker_output = future.result()   
                            scrape_status = worker_output.get("result", {}).get("status",
                                                                                "unknown_error_from_worker")   
                            brands_count = worker_output.get("result", {}).get("brands_processed_count", 0)   
                            message = worker_output.get("result", {}).get("message", scrape_status)   

                            logging.info(
                                f"Worker cho ng√†y {processed_date.strftime('%Y-%m-%d')} ƒë√£ HO√ÄN TH√ÄNH. Status: {scrape_status}, Brands: {brands_count}, Msg: {message}")   

                            if scrape_status in ["completed_all_pages", "no_data_on_first_page"]:   
                                logging.info(
                                    f"‚úÖ HO√ÄN T·∫§T X·ª¨ L√ù D·ªÆ LI·ªÜU CHO NG√ÄY: {processed_date.strftime('%Y-%m-%d')}.")   
                                save_control_state(CONTROL_STATE_FILE_PATH, processed_date)   
                                clear_page_state_for_day(PAGE_STATE_FILE_PATH, processed_date,
                                                         page_state_lock)   
                                days_processed_in_this_session += 1   
                            elif scrape_status == "worker_crash":   
                                logging.error(
                                    f"üíÄ Worker cho ng√†y {processed_date.strftime('%Y-%m-%d')} b·ªã CRASH. L√Ω do: {message}")   
                            else:   
                                logging.warning(
                                    f"Ng√†y {processed_date.strftime('%Y-%m-%d')} ch∆∞a ho√†n th√†nh b·ªüi worker. "   
                                    f"L√Ω do: {message}.")   
                        except Exception as e:   
                            logging.error(
                                f"L·ªói khi l·∫•y k·∫øt qu·∫£ t·ª´ future cho ng√†y {processed_date.strftime('%Y-%m-%d')}: {e}",
                                exc_info=True)   

                    while len(active_futures) < NUM_PROCESSES:   
                        if current_day_to_process > overall_end_date:   
                            logging.info(
                                f"ƒê√£ ƒë·∫°t ƒë·∫øn ng√†y k·∫øt th√∫c t·ªïng th·ªÉ ({overall_end_date.strftime('%Y-%m-%d')}). Kh√¥ng th√™m task m·ªõi.")   
                            break   

                        logging.info(
                            f"--- Chu·∫©n b·ªã g·ª≠i task cho ng√†y: {current_day_to_process.strftime('%Y-%m-%d')} ---")   

                        worker_func_partial = partial(   
                            scrape_day_worker,   
                            db_url=settings.DATABASE_URL,   
                            page_state_file_path=PAGE_STATE_FILE_PATH,   
                            page_state_lock=page_state_lock,   
                            media_physical_dir_worker=MEDIA_PHYSICAL_DIR   
                        )   

                        future = executor.submit(worker_func_partial, current_day_to_process)   
                        active_futures[future] = current_day_to_process   
                        logging.info(
                            f"ƒê√£ g·ª≠i task x·ª≠ l√Ω ng√†y {current_day_to_process.strftime('%Y-%m-%d')} cho worker.")   

                        current_day_to_process += timedelta(days=1)   

                        if len(active_futures) == NUM_PROCESSES:   
                            logging.info(
                                f"ƒê√£ s·ª≠ d·ª•ng h·∫øt {NUM_PROCESSES} worker slots. Ch·ªù worker ho√†n th√†nh...")   

                    if not active_futures and current_day_to_process > overall_end_date:   
                        logging.info(
                            "Kh√¥ng c√≤n task n√†o ƒëang ch·∫°y v√† ƒë√£ x·ª≠ l√Ω h·∫øt c√°c ng√†y theo k·∫ø ho·∫°ch.")   
                        break   

                    await asyncio.sleep(0.1)

                    if (asyncio.get_event_loop().time() - session_start_time_ns) >= RUN_DURATION_SECONDS:   
                        logging.info(
                            "H·∫øt th·ªùi gian phi√™n l√†m vi·ªác quy ƒë·ªãnh. Ch·ªù c√°c task ƒëang ch·∫°y ho√†n th√†nh...")   
                        break   

                if active_futures:   
                    logging.info(
                        f"H·∫øt gi·ªù l√†m vi·ªác, ch·ªù {len(active_futures)} tasks ƒëang ch·∫°y ho√†n th√†nh...")   
                    # Chuy·ªÉn active_futures.keys() th√†nh list ƒë·ªÉ tr√°nh l·ªói thay ƒë·ªïi k√≠ch th∆∞·ªõc trong khi l·∫∑p
                    for future in as_completed(list(active_futures.keys())):   
                        processed_date = active_futures.pop(future)   
                        try:   
                            worker_output = future.result()   
                            scrape_status = worker_output.get("result", {}).get("status",
                                                                                "unknown_error_from_worker")   
                            brands_count = worker_output.get("result", {}).get("brands_processed_count", 0)   
                            message = worker_output.get("result", {}).get("message", scrape_status)   
                            logging.info(
                                f"Worker (sau gi·ªù) cho ng√†y {processed_date.strftime('%Y-%m-%d')} ƒë√£ HO√ÄN TH√ÄNH. Status: {scrape_status}, Brands: {brands_count}, Msg: {message}")   

                            if scrape_status in ["completed_all_pages", "no_data_on_first_page"]:   
                                save_control_state(CONTROL_STATE_FILE_PATH, processed_date)   
                                clear_page_state_for_day(PAGE_STATE_FILE_PATH, processed_date,
                                                         page_state_lock)   
                                days_processed_in_this_session += 1   
                            elif scrape_status == "worker_crash":   
                                logging.error(
                                    f"üíÄ Worker (sau gi·ªù) cho ng√†y {processed_date.strftime('%Y-%m-%d')} b·ªã CRASH. L√Ω do: {message}")   
                            else:   
                                logging.warning(
                                    f"Ng√†y (sau gi·ªù) {processed_date.strftime('%Y-%m-%d')} ch∆∞a ho√†n th√†nh b·ªüi worker. L√Ω do: {message}.")   
                        except Exception as e:   
                            logging.error(
                                f"L·ªói khi l·∫•y k·∫øt qu·∫£ t·ª´ future (sau gi·ªù) cho ng√†y {processed_date.strftime('%Y-%m-%d')}: {e}",
                                exc_info=True)   
                    active_futures.clear()   

                if days_processed_in_this_session == 0 and current_day_to_process <= overall_end_date:   
                    logging.info(f"Phi√™n l√†m vi·ªác k·∫øt th√∫c m√† kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c ng√†y n√†o m·ªõi.")   
                elif current_day_to_process > overall_end_date and not active_futures:   
                    logging.info(
                        f"ƒê√£ x·ª≠ l√Ω h·∫øt t·∫•t c·∫£ c√°c ng√†y cho ƒë·∫øn {overall_end_date.strftime('%Y-%m-%d')}. D·ª´ng ch∆∞∆°ng tr√¨nh.")   
                    break   

                logging.info(
                    f"====== K·∫æT TH√öC PHI√äN L√ÄM VI·ªÜC l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")   

                if current_day_to_process > overall_end_date and not active_futures:   
                    break   

                logging.info(
                    f"====== B·∫ÆT ƒê·∫¶U NGH·ªà NG∆†I ({settings.PAUSE_DURATION_MINUTES} PH√öT) l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")   
                await asyncio.sleep(PAUSE_DURATION_SECONDS)   
                logging.info(
                    f"====== K·∫æT TH√öC NGH·ªà NG∆†I l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")   

                current_day_to_process = get_next_day_to_process()

try:
    setup_database_schema()
except Exception as e_db_setup:
    logging.critical(f"Kh√¥ng th·ªÉ thi·∫øt l·∫≠p schema database, d·ª´ng ·ª©ng d·ª•ng: {e_db_setup}")
    exit(1) # D·ª´ng h·∫≥n n·∫øu kh√¥ng th·ªÉ setup DB


async def main_async_runner():   
    await daily_scraping_manager()   


if __name__ == "__main__":
    try:   
        asyncio.run(main_async_runner())   
    except KeyboardInterrupt:   
        logging.info("Tool b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng (Ctrl+C).")   
    except Exception as e:   
        logging.critical(f"L·ªói nghi√™m tr·ªçng kh√¥ng b·∫Øt ƒë∆∞·ª£c ·ªü ph·∫°m vi cao nh·∫•t (main_async_runner): {e}",
                         exc_info=True)   
    finally:   
        logging.info("Ch∆∞∆°ng tr√¨nh k·∫øt th√∫c.")   