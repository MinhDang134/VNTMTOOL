import asyncio
from datetime import datetime, timedelta, date as date_type
import os
from src.tools.config import settings
from src.tools.service import ScraperService
from src.tools.database import get_session
from src.tools.state_manager import (
    load_scrape_state, logging, save_scrape_state,
    load_control_state, save_control_state, get_control_state_path,
    clear_page_state_for_day
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scraper_activity.log", mode='a', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
MEDIA_PHYSICAL_DIR = os.path.join(PROJECT_ROOT, "media_root", "brand_images")
os.makedirs(MEDIA_PHYSICAL_DIR, exist_ok=True)
logging.info(f"Th∆∞ m·ª•c l∆∞u tr·ªØ media: {MEDIA_PHYSICAL_DIR}")


PAGE_STATE_FILE_PATH = os.path.join(PROJECT_ROOT, "scraper_page_state.json")
CONTROL_STATE_FILE_PATH = get_control_state_path(PROJECT_ROOT)

RUN_DURATION_SECONDS = settings.RUN_DURATION_MINUTES * 60
PAUSE_DURATION_SECONDS = settings.PAUSE_DURATION_MINUTES * 60


def get_next_day_to_process() -> date_type:  # n·ªôi dung t·ª´ gemini (to√†n b·ªô h√†m m·ªõi)
    """X√°c ƒë·ªãnh ng√†y ti·∫øp theo c·∫ßn x·ª≠ l√Ω d·ª±a tr√™n tr·∫°ng th√°i ƒë√£ l∆∞u ho·∫∑c c·∫•u h√¨nh."""
    control_state = load_control_state(CONTROL_STATE_FILE_PATH)
    last_completed_str = control_state.get("last_fully_completed_day")

    if last_completed_str:
        try:
            last_completed_date = datetime.strptime(last_completed_str, "%Y-%m-%d").date()
            next_day = last_completed_date + timedelta(days=1)  # n·ªôi dung t·ª´ gemini
            logging.info(
                f"Ng√†y cu·ªëi c√πng ho√†n th√†nh ƒë∆∞·ª£c ghi nh·∫≠n: {last_completed_str}. Ng√†y ti·∫øp theo ƒë·ªÉ x·ª≠ l√Ω: {next_day.strftime('%Y-%m-%d')}")
            return next_day
        except ValueError:
            logging.warning(
                f"ƒê·ªãnh d·∫°ng ng√†y trong file tr·∫°ng th√°i ƒëi·ªÅu khi·ªÉn kh√¥ng h·ª£p l·ªá: '{last_completed_str}'. S·∫Ω b·∫Øt ƒë·∫ßu t·ª´ ng√†y c·∫•u h√¨nh.")

    # N·∫øu kh√¥ng c√≥ tr·∫°ng th√°i ho·∫∑c l·ªói, b·∫Øt ƒë·∫ßu t·ª´ ng√†y c·∫•u h√¨nh trong settings
    start_date = date_type(
        settings.INITIAL_SCRAPE_START_YEAR,
        settings.INITIAL_SCRAPE_START_MOTH,
        settings.INITIAL_SCRAPE_START_DAY
    )
    logging.info(
        f"Kh√¥ng c√≥ tr·∫°ng th√°i ng√†y ho√†n th√†nh h·ª£p l·ªá. B·∫Øt ƒë·∫ßu t·ª´ ng√†y c·∫•u h√¨nh m·∫∑c ƒë·ªãnh: {start_date.strftime('%Y-%m-%d')}")
    return start_date


def get_overall_end_date() -> date_type:
    """L·∫•y ng√†y k·∫øt th√∫c t·ªïng th·ªÉ c·ªßa qu√° tr√¨nh scrape t·ª´ settings, ho·∫∑c m·∫∑c ƒë·ªãnh l√† ng√†y h√¥m qua."""
    if settings.OVERALL_SCRAPE_END_YEAR and settings.OVERALL_SCRAPE_END_MOTH and settings.OVERALL_SCRAPE_END_DAY:
        try:
            return date_type(
                settings.OVERALL_SCRAPE_END_YEAR,
                settings.OVERALL_SCRAPE_END_MOTH,
                settings.OVERALL_SCRAPE_END_DAY
            )
        except (TypeError,
                ValueError) as e:
            logging.warning(
                f"M·ªôt s·ªë gi√° tr·ªã OVERALL_SCRAPE_END trong settings kh√¥ng h·ª£p l·ªá ho·∫∑c thi·∫øu: {e}. S·∫Ω m·∫∑c ƒë·ªãnh l√† ng√†y h√¥m qua.")
    return datetime.now().date() - timedelta(days=1)


async def daily_scraping_task():
    """T√°c v·ª• ch√≠nh, qu·∫£n l√Ω vi·ªác c√†o d·ªØ li·ªáu theo t·ª´ng ng√†y v√† chu k·ª≥ ch·∫°y/ngh·ªâ."""
    scraper = ScraperService()
    current_day_to_process = get_next_day_to_process()
    overall_end_date = get_overall_end_date()

    while True:
        logging.info(
            f"====== B·∫ÆT ƒê·∫¶U PHI√äN L√ÄM VI·ªÜC ({settings.RUN_DURATION_MINUTES} PH√öT) l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")
        session_start_time = asyncio.get_event_loop().time()
        session_processed_any_day = False

        # V√≤ng l·∫∑p x·ª≠ l√Ω c√°c ng√†y trong m·ªôt phi√™n l√†m vi·ªác (gi·ªõi h·∫°n b·ªüi RUN_DURATION_SECONDS)
        while (asyncio.get_event_loop().time() - session_start_time) < RUN_DURATION_SECONDS:
            if current_day_to_process > overall_end_date:
                logging.info(f"ƒê√£ ƒë·∫°t ƒë·∫øn ng√†y k·∫øt th√∫c t·ªïng th·ªÉ ({overall_end_date.strftime('%Y-%m-%d')}). "
                             f"T·∫°m d·ª´ng phi√™n l√†m vi·ªác, s·∫Ω ki·ªÉm tra l·∫°i sau giai ƒëo·∫°n ngh·ªâ.")
                break
            logging.info(
                f"--- Chu·∫©n b·ªã x·ª≠ l√Ω cho ng√†y: {current_day_to_process.strftime('%Y-%m-%d')} ---")

            # Key cho tr·∫°ng th√°i trang c·ªßa ng√†y hi·ªán t·∫°i (v√≠ d·ª•: "brands_2023-01-15_2023-01-15")
            day_key = f"brands_{current_day_to_process.strftime('%Y-%m-%d')}_{current_day_to_process.strftime('%Y-%m-%d')}"
            initial_page_for_this_day = load_scrape_state(PAGE_STATE_FILE_PATH, day_key)  # n·ªôi dung t·ª´ gemini

            scrape_result = {"status": "not_started", "brands_processed_count": 0,
                             "message": "Ch∆∞a b·∫Øt ƒë·∫ßu"}
            try:
                with get_session() as session:
                    def state_saver_callback_for_day(page_just_completed: int):
                        save_scrape_state(PAGE_STATE_FILE_PATH, day_key, page_just_completed)

                    logging.info(
                        f"üöÄ B·∫Øt ƒë·∫ßu scrape cho ng√†y [{current_day_to_process.strftime('%Y-%m-%d')}], t·ª´ trang {initial_page_for_this_day}.")
                    scrape_result = await scraper.scrape_by_date_range(
                        start_date=current_day_to_process,
                        end_date=current_day_to_process,
                        session=session,
                        initial_start_page=initial_page_for_this_day,
                        state_save_callback=state_saver_callback_for_day
                    )

            except Exception as e_main_scope:
                logging.error(
                    f"L·ªói kh√¥ng mong mu·ªën trong qu√° tr√¨nh x·ª≠ l√Ω ng√†y {current_day_to_process.strftime('%Y-%m-%d')}: {e_main_scope}",
                    exc_info=True)
                scrape_result = {"status": "critical_error", "brands_processed_count": 0,
                                 "message": str(e_main_scope)}
                await asyncio.sleep(5)
                break  # D·ª´ng phi√™n l√†m vi·ªác n·∫øu c√≥ l·ªói nghi√™m tr·ªçng # n·ªôi dung t·ª´ gemini

            session_processed_any_day = True
            status = scrape_result.get("status", "unknown_error")
            brands_count = scrape_result.get("brands_processed_count", 0)
            message = scrape_result.get("message", status)

            if status in ["completed_all_pages", "no_data_on_first_page"]:
                logging.info(f"‚úÖ HO√ÄN T·∫§T X·ª¨ L√ù D·ªÆ LI·ªÜU CHO NG√ÄY: {current_day_to_process.strftime('%Y-%m-%d')}. "
                             f"S·ªë nh√£n hi·ªáu ƒë√£ x·ª≠ l√Ω: {brands_count}. L√Ω do: {message}")
                save_control_state(CONTROL_STATE_FILE_PATH,
                                   current_day_to_process)  # L∆∞u ng√†y n√†y ƒë√£ ho√†n th√†nh # n·ªôi dung t·ª´ gemini
                clear_page_state_for_day(PAGE_STATE_FILE_PATH,
                                         current_day_to_process)  # X√≥a tr·∫°ng th√°i trang c·ªßa ng√†y ƒë√£ xong # n·ªôi dung t·ª´ gemini
                current_day_to_process += timedelta(days=1)  # Chuy·ªÉn sang ng√†y ti·∫øp theo ƒë·ªÉ x·ª≠ l√Ω # n·ªôi dung t·ª´ gemini
            else:  # C√°c tr∆∞·ªùng h·ª£p l·ªói (request_error, soup_error, db_commit_error, critical_error) ho·∫∑c ng√†y ch∆∞a xong # n·ªôi dung t·ª´ gemini
                logging.warning(f"Ng√†y {current_day_to_process.strftime('%Y-%m-%d')} ch∆∞a ho√†n th√†nh ho·∫∑c g·∫∑p l·ªói. "
                                f"L√Ω do: {message}. S·∫Ω th·ª≠ l·∫°i trong phi√™n l√†m vi·ªác ti·∫øp theo.")
                # Tr·∫°ng th√°i trang c·ªßa ng√†y n√†y ƒë√£ ƒë∆∞·ª£c state_save_callback l∆∞u.
                # D·ª´ng x·ª≠ l√Ω c√°c ng√†y ti·∫øp theo trong phi√™n n√†y, chuy·ªÉn sang giai ƒëo·∫°n ngh·ªâ.
                break  # Tho√°t kh·ªèi v√≤ng l·∫∑p x·ª≠ l√Ω c√°c ng√†y trong phi√™n hi·ªán t·∫°i # n·ªôi dung t·ª´ gemini

            # Ki·ªÉm tra l·∫°i th·ªùi gian phi√™n l√†m vi·ªác sau m·ªói ng√†y x·ª≠ l√Ω xong
            if (asyncio.get_event_loop().time() - session_start_time) >= RUN_DURATION_SECONDS:
                logging.info("H·∫øt th·ªùi gian phi√™n l√†m vi·ªác quy ƒë·ªãnh.")
                break  # Tho√°t kh·ªèi v√≤ng l·∫∑p x·ª≠ l√Ω c√°c ng√†y trong phi√™n

            await asyncio.sleep(1)  # Ngh·ªâ ng·∫Øn gi·ªØa c√°c ng√†y n·∫øu c√≤n th·ªùi gian trong phi√™n

        # Logging k·∫øt th√∫c phi√™n l√†m vi·ªác
        if not session_processed_any_day and current_day_to_process <= overall_end_date:
            logging.info(
                f"Phi√™n l√†m vi·ªác k·∫øt th√∫c m√† kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c ng√†y n√†o (c√≥ th·ªÉ do l·ªói ban ƒë·∫ßu ho·∫∑c ƒë√£ ƒë·∫°t gi·ªõi h·∫°n ng√†y x·ª≠ l√Ω).")
        elif not session_processed_any_day and current_day_to_process > overall_end_date:
            logging.info(
                f"ƒê√£ ƒë·∫°t ng√†y k·∫øt th√∫c t·ªïng th·ªÉ. S·∫Ω ki·ªÉm tra l·∫°i sau giai ƒëo·∫°n ngh·ªâ ng∆°i.")

        logging.info(
            f"====== K·∫æT TH√öC PHI√äN L√ÄM VI·ªÜC l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")
        logging.info(
            f"====== B·∫ÆT ƒê·∫¶U NGH·ªà NG∆†I ({settings.PAUSE_DURATION_MINUTES} PH√öT) l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")
        await asyncio.sleep(PAUSE_DURATION_SECONDS)
        logging.info(
            f"====== K·∫æT TH√öC NGH·ªà NG∆†I l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")

        # C·∫≠p nh·∫≠t l·∫°i ng√†y c·∫ßn x·ª≠ l√Ω ph√≤ng tr∆∞·ªùng h·ª£p control_state thay ƒë·ªïi b·ªüi ti·∫øn tr√¨nh kh√°c (hi·∫øm)
        # ho·∫∑c ƒë·ªÉ ƒë·∫£m b·∫£o logic b·∫Øt ƒë·∫ßu ng√†y m·ªõi l√† ch√≠nh x√°c sau khi ngh·ªâ.
        new_next_day = get_next_day_to_process()  # n·ªôi dung t·ª´ gemini
        if new_next_day < current_day_to_process and status in ["completed_all_pages",
                                                                "no_data_on_first_page"]:
            # ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† ng√†y v·ª´a x·ª≠ l√Ω xong ƒë√£ ƒë∆∞·ª£c l∆∞u ƒë√∫ng, v√† get_next_day_to_process ƒë√£ t√≠nh ng√†y ti·∫øp theo
            # Kh√¥ng c·∫ßn l√†m g√¨ current_day_to_process ƒë√£ ƒë∆∞·ª£c tƒÉng l√™n
            pass
        else:
            current_day_to_process = new_next_day


async def main():  # n·ªôi dung t·ª´ gemini (h√†m main m·ªõi)
    # ch·∫°y check_pending_brands ·ªü ƒë√¢y n·∫øu mu·ªën, v√≠ d·ª• t·∫°o m·ªôt task ch·∫°y ƒë·ªãnh k·ª≥
    # async def periodic_pending_check_task():
    #     scraper_for_pending = ScraperService()
    #     while True:
    #         logging.info("B·∫Øt ƒë·∫ßu t√°c v·ª• ki·ªÉm tra ƒë·ªãnh k·ª≥ c√°c ƒë∆°n ƒëang ch·ªù x·ª≠ l√Ω...")
    #         try:
    #             with get_session() as session:
    #                 await scraper_for_pending.check_pending_brands(session)
    #         except Exception as e_pending:
    #             logging.error(f"L·ªói trong t√°c v·ª• ki·ªÉm tra ƒë·ªãnh k·ª≥ c√°c ƒë∆°n ƒëang ch·ªù: {e_pending}", exc_info=True)
    #         await asyncio.sleep(3600) # V√≠ d·ª•: ch·∫°y m·ªói gi·ªù
    #
    # asyncio.create_task(periodic_pending_check_task())
    await daily_scraping_task()


if __name__ == "__main__":
    try:  #  (th√™m try-except)
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("Tool b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng (Ctrl+C).")
    except Exception as e:
        logging.critical(f"L·ªói nghi√™m tr·ªçng kh√¥ng b·∫Øt ƒë∆∞·ª£c ·ªü ph·∫°m vi cao nh·∫•t (main): {e}",
                         exc_info=True)