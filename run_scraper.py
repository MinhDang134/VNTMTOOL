import asyncio
from datetime import datetime, timedelta, date as date_type
import os
from src.tools.config import settings
from src.tools.service import ScraperService
from src.tools.database import get_session
from src.tools.state_manager import (
    load_scrape_state, logging, save_scrape_state,
    load_control_state, save_control_state, get_control_state_path,
    clear_page_state_for_day
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scraper_activity.log", mode='a', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
MEDIA_PHYSICAL_DIR = os.path.join(PROJECT_ROOT, "media_root", "brand_images")
os.makedirs(MEDIA_PHYSICAL_DIR, exist_ok=True)
logging.info(f"Th∆∞ m·ª•c l∆∞u tr·ªØ media: {MEDIA_PHYSICAL_DIR}")


PAGE_STATE_FILE_PATH = os.path.join(PROJECT_ROOT, "scraper_page_state.json")
CONTROL_STATE_FILE_PATH = get_control_state_path(PROJECT_ROOT)

RUN_DURATION_SECONDS = settings.RUN_DURATION_MINUTES * 60
PAUSE_DURATION_SECONDS = settings.PAUSE_DURATION_MINUTES * 60


def get_next_day_to_process() -> date_type:  #   (to√†n b·ªô h√†m m·ªõi)
    """X√°c ƒë·ªãnh ng√†y ti·∫øp theo c·∫ßn x·ª≠ l√Ω d·ª±a tr√™n tr·∫°ng th√°i ƒë√£ l∆∞u ho·∫∑c c·∫•u h√¨nh."""
    control_state = load_control_state(CONTROL_STATE_FILE_PATH)
    last_completed_str = control_state.get("last_fully_completed_day")

    if last_completed_str:
        try:
            last_completed_date = datetime.strptime(last_completed_str, "%Y-%m-%d").date()
            next_day = last_completed_date + timedelta(days=1)
            logging.info(
                f"Ng√†y cu·ªëi c√πng ho√†n th√†nh ƒë∆∞·ª£c ghi nh·∫≠n: {last_completed_str}. Ng√†y ti·∫øp theo ƒë·ªÉ x·ª≠ l√Ω: {next_day.strftime('%Y-%m-%d')}")
            return next_day
        except ValueError:
            logging.warning(
                f"ƒê·ªãnh d·∫°ng ng√†y trong file tr·∫°ng th√°i ƒëi·ªÅu khi·ªÉn kh√¥ng h·ª£p l·ªá: '{last_completed_str}'. S·∫Ω b·∫Øt ƒë·∫ßu t·ª´ ng√†y c·∫•u h√¨nh.")

    # N·∫øu kh√¥ng c√≥ tr·∫°ng th√°i ho·∫∑c l·ªói, b·∫Øt ƒë·∫ßu t·ª´ ng√†y c·∫•u h√¨nh trong settings
    start_date = date_type(
        settings.INITIAL_SCRAPE_START_YEAR,
        settings.INITIAL_SCRAPE_START_MOTH,
        settings.INITIAL_SCRAPE_START_DAY
    )
    logging.info(
        f"Kh√¥ng c√≥ tr·∫°ng th√°i ng√†y ho√†n th√†nh h·ª£p l·ªá. B·∫Øt ƒë·∫ßu t·ª´ ng√†y c·∫•u h√¨nh m·∫∑c ƒë·ªãnh: {start_date.strftime('%Y-%m-%d')}")
    return start_date


def get_overall_end_date() -> date_type:
    """L·∫•y ng√†y k·∫øt th√∫c t·ªïng th·ªÉ c·ªßa qu√° tr√¨nh scrape t·ª´ settings, ho·∫∑c m·∫∑c ƒë·ªãnh l√† ng√†y h√¥m qua."""
    if settings.OVERALL_SCRAPE_END_YEAR and settings.OVERALL_SCRAPE_END_MOTH and settings.OVERALL_SCRAPE_END_DAY:
        try:
            return date_type(
                settings.OVERALL_SCRAPE_END_YEAR,
                settings.OVERALL_SCRAPE_END_MOTH,
                settings.OVERALL_SCRAPE_END_DAY
            )
        except (TypeError,
                ValueError) as e:
            logging.warning(
                f"M·ªôt s·ªë gi√° tr·ªã OVERALL_SCRAPE_END trong settings kh√¥ng h·ª£p l·ªá ho·∫∑c thi·∫øu: {e}. S·∫Ω m·∫∑c ƒë·ªãnh l√† ng√†y h√¥m qua.")
    return datetime.now().date() - timedelta(days=1)


async def daily_scraping_task():
    """T√°c v·ª• ch√≠nh, qu·∫£n l√Ω vi·ªác c√†o d·ªØ li·ªáu theo t·ª´ng ng√†y v√† chu k·ª≥ ch·∫°y/ngh·ªâ."""
    scraper = ScraperService()
    current_day_to_process = get_next_day_to_process()
    overall_end_date = get_overall_end_date()

    while True:
        logging.info(
            f"====== B·∫ÆT ƒê·∫¶U PHI√äN L√ÄM VI·ªÜC ({settings.RUN_DURATION_MINUTES} PH√öT) l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")
        session_start_time = asyncio.get_event_loop().time()
        session_processed_any_day = False


        while (asyncio.get_event_loop().time() - session_start_time) < RUN_DURATION_SECONDS:
            if current_day_to_process > overall_end_date:
                logging.info(f"ƒê√£ ƒë·∫°t ƒë·∫øn ng√†y k·∫øt th√∫c t·ªïng th·ªÉ ({overall_end_date.strftime('%Y-%m-%d')}). "
                             f"T·∫°m d·ª´ng phi√™n l√†m vi·ªác, s·∫Ω ki·ªÉm tra l·∫°i sau giai ƒëo·∫°n ngh·ªâ.")
                break
            logging.info(
                f"--- Chu·∫©n b·ªã x·ª≠ l√Ω cho ng√†y: {current_day_to_process.strftime('%Y-%m-%d')} ---")


            day_key = f"brands_{current_day_to_process.strftime('%Y-%m-%d')}_{current_day_to_process.strftime('%Y-%m-%d')}"
            initial_page_for_this_day = load_scrape_state(PAGE_STATE_FILE_PATH, day_key)  #  

            scrape_result = {"status": "not_started", "brands_processed_count": 0,
                             "message": "Ch∆∞a b·∫Øt ƒë·∫ßu"}
            try:
                with get_session() as session:
                    def state_saver_callback_for_day(page_just_completed: int):
                        save_scrape_state(PAGE_STATE_FILE_PATH, day_key, page_just_completed)

                    logging.info(
                        f"üöÄ B·∫Øt ƒë·∫ßu scrape cho ng√†y [{current_day_to_process.strftime('%Y-%m-%d')}], t·ª´ trang {initial_page_for_this_day}.")
                    scrape_result = await scraper.scrape_by_date_range(
                        start_date=current_day_to_process,
                        end_date=current_day_to_process,
                        session=session,
                        initial_start_page=initial_page_for_this_day,
                        state_save_callback=state_saver_callback_for_day
                    )

            except Exception as e_main_scope:
                logging.error(
                    f"L·ªói kh√¥ng mong mu·ªën trong qu√° tr√¨nh x·ª≠ l√Ω ng√†y {current_day_to_process.strftime('%Y-%m-%d')}: {e_main_scope}",
                    exc_info=True)
                await asyncio.sleep(5)
                break

            session_processed_any_day = True
            status = scrape_result.get("status", "unknown_error")
            brands_count = scrape_result.get("brands_processed_count", 0)
            message = scrape_result.get("message", status)

            if status in ["completed_all_pages", "no_data_on_first_page"]:
                logging.info(f"‚úÖ HO√ÄN T·∫§T X·ª¨ L√ù D·ªÆ LI·ªÜU CHO NG√ÄY: {current_day_to_process.strftime('%Y-%m-%d')}. "
                             f"S·ªë nh√£n hi·ªáu ƒë√£ x·ª≠ l√Ω: {brands_count}. L√Ω do: {message}")
                save_control_state(CONTROL_STATE_FILE_PATH,current_day_to_process)
                clear_page_state_for_day(PAGE_STATE_FILE_PATH,
                                         current_day_to_process)
                current_day_to_process += timedelta(days=1)
            else:
                logging.warning(f"Ng√†y {current_day_to_process.strftime('%Y-%m-%d')} ch∆∞a ho√†n th√†nh ho·∫∑c g·∫∑p l·ªói. "
                                f"L√Ω do: {message}. S·∫Ω th·ª≠ l·∫°i trong phi√™n l√†m vi·ªác ti·∫øp theo.")
                break


            if (asyncio.get_event_loop().time() - session_start_time) >= RUN_DURATION_SECONDS:
                logging.info("H·∫øt th·ªùi gian phi√™n l√†m vi·ªác quy ƒë·ªãnh.")
                break

            await asyncio.sleep(1)


        if not session_processed_any_day and current_day_to_process <= overall_end_date:
            logging.info(
                f"Phi√™n l√†m vi·ªác k·∫øt th√∫c m√† kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c ng√†y n√†o (c√≥ th·ªÉ do l·ªói ban ƒë·∫ßu ho·∫∑c ƒë√£ ƒë·∫°t gi·ªõi h·∫°n ng√†y x·ª≠ l√Ω).")
        elif not session_processed_any_day and current_day_to_process > overall_end_date:
            logging.info(
                f"ƒê√£ ƒë·∫°t ng√†y k·∫øt th√∫c t·ªïng th·ªÉ. S·∫Ω ki·ªÉm tra l·∫°i sau giai ƒëo·∫°n ngh·ªâ ng∆°i.")

        logging.info(
            f"====== K·∫æT TH√öC PHI√äN L√ÄM VI·ªÜC l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")
        logging.info(
            f"====== B·∫ÆT ƒê·∫¶U NGH·ªà NG∆†I ({settings.PAUSE_DURATION_MINUTES} PH√öT) l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")
        await asyncio.sleep(PAUSE_DURATION_SECONDS)
        logging.info(
            f"====== K·∫æT TH√öC NGH·ªà NG∆†I l√∫c {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")

        new_next_day = get_next_day_to_process()  #  
        if new_next_day < current_day_to_process and status in ["completed_all_pages","no_data_on_first_page"]:
            pass
        else:
            current_day_to_process = new_next_day


async def main():
    await daily_scraping_task()


if __name__ == "__main__":
    try:  #  (th√™m try-except)
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("Tool b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng (Ctrl+C).")
    except Exception as e:
        logging.critical(f"L·ªói nghi√™m tr·ªçng kh√¥ng b·∫Øt ƒë∆∞·ª£c ·ªü ph·∫°m vi cao nh·∫•t (main): {e}",
                         exc_info=True)