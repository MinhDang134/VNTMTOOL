import asyncio
from datetime import datetime
import logging
import os
from src.tools.service import ScraperService
from src.tools.database import get_session, ensure_partition_exists # ensure_partition_exists cÃ³ váº» khÃ´ng cáº§n á»Ÿ Ä‘Ã¢y ná»¯a

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
MEDIA_PHYSICAL_DIR = os.path.join(PROJECT_ROOT, "media_root", "brand_images")
os.makedirs(MEDIA_PHYSICAL_DIR, exist_ok=True) # Ráº¥t tá»‘t! Äáº£m báº£o thÆ° má»¥c tá»“n táº¡i
logging.info(f"ThÆ° má»¥c lÆ°u trá»¯ media: {MEDIA_PHYSICAL_DIR}")

async def run_scraper():
    scraper = ScraperService()
    start_date = datetime(2022, 2, 1)
    end_date = datetime(2022, 2, 28)

    with get_session() as session: # Context manager nÃ y sáº½ tá»± rollback náº¿u cÃ³ lá»—i khÃ´ng Ä‘Æ°á»£c xá»­ lÃ½
        try:
            logging.info(f"ğŸš€ Báº¯t Ä‘áº§u scrape tá»« {start_date.date()} Ä‘áº¿n {end_date.date()}")
            brands = await scraper.scrape_by_date_range(start_date, end_date, session)

            # ---- PHáº¦N NÃ€Y NÃŠN Bá» ÄI ----
            # LÃ½ do: scrape_by_date_range trong service.py Ä‘Ã£ gá»i ensure_partition_exists
            # vÃ  bulk_create (bao gá»“m cáº£ session.add vÃ  session.commit).
            # Viá»‡c gá»i láº¡i á»Ÿ Ä‘Ã¢y sáº½ thá»«a vÃ  cÃ³ thá»ƒ gÃ¢y lá»—i.
            #
            # for brand in brands:
            #     ensure_partition_exists(brand.application_date) # <- ÄÃ£ lÃ m trong service
            #     session.add(brand) # <- ÄÃ£ lÃ m trong service (qua bulk_create)
            # session.commit() # <- ÄÃ£ lÃ m trong service (qua bulk_create)
            # ---- Káº¾T THÃšC PHáº¦N NÃŠN Bá» ----

            if brands is not None: # brands lÃ  danh sÃ¡ch tráº£ vá» tá»« service
                logging.info(f"âœ… QuÃ¡ trÃ¬nh scrape hoÃ n táº¥t. Service Ä‘Ã£ xá»­ lÃ½ {len(brands)} nhÃ£n hiá»‡u.")
            else:
                # TrÆ°á»ng há»£p nÃ y xáº£y ra náº¿u scrape_by_date_range bá»‹ lá»—i vÃ  tráº£ vá» None trÆ°á»›c khi xá»­ lÃ½ brands
                logging.warning("â„¹ï¸ QuÃ¡ trÃ¬nh scrape cÃ³ thá»ƒ Ä‘Ã£ gáº·p lá»—i vÃ  khÃ´ng tráº£ vá» danh sÃ¡ch nhÃ£n hiá»‡u.")

        except Exception as e:
            # Lá»—i á»Ÿ Ä‘Ã¢y thÆ°á»ng lÃ  lá»—i káº¿t ná»‘i DB ban Ä‘áº§u, hoáº·c lá»—i khÃ´ng mong muá»‘n bÃªn ngoÃ i service.
            # Service Ä‘Ã£ cÃ³ try-except riÃªng cho logic scrape.
            logging.error(f"âŒ Lá»—i nghiÃªm trá»ng trong quÃ¡ trÃ¬nh cháº¡y scraper: {str(e)}", exc_info=True)
            # session.rollback() sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng gá»i bá»Ÿi context manager 'with get_session()' náº¿u cÃ³ lá»—i thoÃ¡t ra khá»i khá»‘i try

if __name__ == "__main__":
    asyncio.run(run_scraper())